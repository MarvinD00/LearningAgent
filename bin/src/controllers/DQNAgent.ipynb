{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tf_agents as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisEnv(tfa.environments.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 0 - move left\n",
    "        # 1 - move right\n",
    "        # 2 - move down\n",
    "        # 3 - rotate\n",
    "        self._action_spec = tfa.specs.array_spec.BoundedArraySpec(\n",
    "            shape = (),\n",
    "            dtype = np.int32,\n",
    "            minimum = 0,\n",
    "            maximum = 3,\n",
    "            name = 'action'\n",
    "        )\n",
    "        self._observation_spec = tfa.specs.array_spec.BoundedArraySpec(\n",
    "            shape = (10,18),\n",
    "            dtype = np.int32,\n",
    "            minimum = 0,\n",
    "            maximum = 17,\n",
    "            name = 'observation'\n",
    "        )\n",
    "        \n",
    "        self._state = np.random.randint(30, 40)\n",
    "        \n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _observe(self):\n",
    "        return np.array([self._position], dtype = np.int32)\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._position = 0\n",
    "        self._step_counter = 0\n",
    "        return tfa.trajectories.time_step.restart(self._observe())\n",
    "    \n",
    "    def _step(self, action):\n",
    "        if abs(self._position) >= 3 or self._step_counter >= 10:\n",
    "            return self.reset()\n",
    "        \n",
    "        self._step_counter += 1\n",
    "        \n",
    "        if action == 0:\n",
    "            self._position -= 1\n",
    "        elif action == 1:\n",
    "            pass\n",
    "        elif action == 2:\n",
    "            self._position += 1\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 (left), 1 (do nothing) or 2 (right). You gave `%s`' % action)\n",
    "        \n",
    "        reward = 0\n",
    "        if self._position >= 3:\n",
    "            reward = 1\n",
    "        elif self._position <= -3 or self._step_counter >= 10:\n",
    "            reward = -1\n",
    "        \n",
    "        if reward != 0:\n",
    "            return tfa.trajectories.time_step.termination(\n",
    "                self._observe(),\n",
    "                reward\n",
    "            )\n",
    "        else:\n",
    "            return tfa.trajectories.time_step.transition(\n",
    "                self._observe(),\n",
    "                reward = 0, \n",
    "                discount = 1.0 \n",
    "            )\n",
    "\n",
    "tfa.environments.utils.validate_py_environment(TetrisEnv(), episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = TetrisEnv()\n",
    "test_py_env = TetrisEnv()\n",
    "train_env = tfa.environments.tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "test_env = tfa.environments.tf_py_environment.TFPyEnvironment(test_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier ist der entgültige Agent. Er muss nur noch mit dem Tetris-Spiel kompatibel gemacht werden. Das heißt:\n",
    "- \"self._state\" muss block_grid entsprechen\n",
    "- \"self._step()\" muss Blöcke bewegen und rotieren können\n",
    "- Restliche Kompatibilitätsprobleme müssen behoben werden sofern welche vorhanden sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = tfa.networks.sequential.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3, activation = None) \n",
    "])\n",
    "\n",
    "agent = tfa.agents.dqn.dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network = q_network,\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    td_errors_loss_fn = tfa.utils.common.element_wise_squared_loss,\n",
    "    n_step_update = 1\n",
    ")\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "agent.train = tfa.utils.common.function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, episodes = 10):\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        time_step = env.reset()\n",
    "        \n",
    "        total_reward += time_step.reward.numpy()[0]\n",
    "        \n",
    "        while not time_step.is_last(): \n",
    "            action_step = policy.action(time_step)\n",
    "            action_tensor = action_step.action\n",
    "            action = action_tensor.numpy()[0]\n",
    "            \n",
    "            time_step = env.step(action)\n",
    "            \n",
    "            total_reward += time_step.reward.numpy()[0]\n",
    "            total_steps += 1\n",
    "        \n",
    "    average_reward = total_reward / episodes\n",
    "    average_ep_length = total_steps / episodes\n",
    "    return average_reward, average_ep_length\n",
    "\n",
    "avg_reward, avg_length = evaluate_policy(test_env, agent.policy)\n",
    "print(\"initial policy gives average reward of %.2f after an average %d steps\" % (avg_reward, avg_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tfa.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = agent.collect_data_spec,\n",
    "    batch_size = train_env.batch_size,\n",
    "    max_length = 10000\n",
    ")\n",
    "\n",
    "replay_dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls = 3, \n",
    "    sample_batch_size = 64,\n",
    "    num_steps = 2\n",
    ").prefetch(3)\n",
    "\n",
    "def record_experience(buffer, time_step, action_step, next_time_step):\n",
    "    buffer.add_batch(\n",
    "        tfa.trajectories.trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    )\n",
    "\n",
    "replay_dataset_iterator = iter(replay_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = train_env.reset()\n",
    "\n",
    "episode_length_history = []\n",
    "reward_history = []\n",
    "\n",
    "for step in range(10 + 1):\n",
    "    \n",
    "    for _ in range(10):\n",
    "        action_step = agent.collect_policy.action(time_step)\n",
    "        \n",
    "        action_tensor = action_step.action\n",
    "        action = action_tensor.numpy()[0]\n",
    "        new_time_step = train_env.step(action)\n",
    "        \n",
    "        reward = new_time_step.reward.numpy()[0]\n",
    "        \n",
    "        record_experience(replay_buffer, time_step, action_step, new_time_step)\n",
    "        \n",
    "        time_step = new_time_step\n",
    "    \n",
    "    training_experience, unused_diagnostics_info = next(replay_dataset_iterator)\n",
    "    \n",
    "    train_step = agent.train(training_experience)\n",
    "    loss = train_step.loss\n",
    "    \n",
    "    print(\"step: %d, loss: %d\" % (step, loss))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        avg_reward, avg_length = evaluate_policy(test_env, agent.policy)\n",
    "        print(\"average reward: %.2f average steps: %d\" % (avg_reward, avg_length))\n",
    "        \n",
    "        reward_history.append(avg_reward)\n",
    "        episode_length_history.append(avg_length)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
